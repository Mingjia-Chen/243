---
title: "An Overview of the MFA package"
author: 
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
\fontsize{12}{12}
```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
options(tibble.print_min = 4L, tibble.print_max = 4L)
library(MFA)
```

This vignette gives an introduction to the model Multiple Factor Analysis and an overview the package MFA developed to apply the model on data.  Most of this information is available scattered throughout the R documentation. This appendix brings it all together in one place. You might find reading this entire vignette helpful to get a broad understanding of what can be done in R using the MFA.

## 1 Introdution to Multiple Factor Analysis
Multiple factor analysis (MFA, also called multiple factorial analysis) is an generalization of principal component analysis (PCA). Its goal is to analyze several data sets of variables collected on the same set of observations, or—as in its dual version—several sets of observations measured on the same set of variables. 

The goals of MFA are (1) to analyze several data sets measured on the same observations; (2) to provide a set of common factor scores (often called ‘compromise factor scores’); and (3) to project each of the original data sets onto the compromise to analyze communalities and discrepancies. 

## 2 When to Use it
MFA is used when several sets of variables have been measured on the same set of observations. The number and/or nature of the variables used to describe the observations can vary from one set of variables to the other, but the observations should be the same in all the data sets.

For example, suppose we have 12 wines evaluated by 10 different wine experts. The different data sets can have the same observations (wines) evaluated by different subjects (wine experts) or groups of sub- jects with different variables (each wine expert evaluates the wines with his/her own set of scales). In this case, the first data set corresponds to the first subject, the second one to the second subject and so on. The goal of the analysis, then, is to evaluate if there is an agreement between the subjects or groups of subjects.

## 3 Main Idea
The general idea behind MFA is to normalize each of the individual data sets so that their first principal component has the same length. There are several terms with regards to MFA: 

**compromise/consensus**: Obtained by combining normalized individual data table into a common representation of the observations.

**factor scores**: The coordinates of the observations on the components. These can be used to plot maps of the ob- servations. 

**partial factor scores**: The positions of the observations ‘as seen by’ each data set. These can be also represented as points in the compromise map. 

**loading**: The quantity that variables contributes a certain amount to each component. This reflects the importance of that variable for this component and can also be used to plot maps of the variables that reflect their association. 

**contributions**: A variation over squared loadings. These evaluate the importance of each variable as the pro- portion of the explained variance of the component by the variable. The contribution of a data table to a component can be obtained by adding the contributions of its variables. These contributions can then be used to draw plots expressing the importance of the data tables in the common solution.


## 4 Step by step tutorial

### Step 1. Preprocesing
Each table was then preprocessed by first centering and normalizing each column such that its mean is equal to 0 and the sum of the square values of all its elements is equal to 1. 
The raw data consist of $K$ data sets collected on the same observations. Each data set is also called a table, a sub-table, or a block. The data for each table are stored in an $I × J_{[k]}$ rectangular data matrix denoted by $\textbf{Y}_{[k]}$, where $I$ is the number of observations and $J_{[k]}$ the number of variables collected on the observations for the $k-th$ table. The total number of variables is denoted by $J$ (i.e., $J = \sum  J_{[k]}$). Each data matrix is, in general, preprocessed (e.g., centered, normalized) and the preprocessed data matrices actually used in the analysis are denoted by $\textbf{X}_{[k]}$.
The $K$ data matrices $\textbf{X}_{[k]}$ are concatenated into the complete $I$ by $J$ data matrix denoted by $\textbf{X}$:
$$\begin{equation}
\textbf{X}=[\textbf{X}_{[1]}|...|\textbf{X}_{[k]}|...|\textbf{X}_{[K]}].
\end{equation}$$
This step was completed by using `scale` function. 

### Step 2. Mass Matrix
A mass, denoted by $m_i$, is assigned to each observation. These masses are collected in the mass vector, denoted by $\textbf{m}$, and in the diagonal elements of the mass matrix denoted by $\textbf{M}$, which is obtained as 
$$\begin{equation}
\textbf{M}=diag\{m\}
\end{equation}$$


### Step 3. Weight Matrix 
The weight matrix gathered by doing standard PCA of each Data Table. Specifically, each table is expressed via its SVD as
$$\begin{equation}
\textbf{X}_{[k]} = \textbf{U}_{[k]} \Gamma_{[k]}\textbf{V}^T_{[k]}\quad \text{with} \quad  \textbf{U}^T_{[k]}\textbf{U}_{[k]} = \textbf{V}^T_{[k]}\textbf{V}_{[k]} = \textbf{I}.
\end{equation}$$
In MFA, the weight of a table is obtained from the first singular value of its PCA. This weight, denoted by αk, is equal to the inverse of the first squared singular value:
$$\begin{equation}
\alpha_k=\frac{1}{\gamma_{1,k}^2}=\gamma_{1,k}^{-2}.
\end{equation}$$
For convenience, the $\alpha$ weights can be gathered in a $J$ by $1$ vector denoted $\textbf{a}$ where each variable is assigned the $\alpha$ weight of the matrix to which it belongs. Specifically, $\textbf{a}$ is constructed as:
$$\begin{equation}
a=[\alpha_1\textbf{1}_{[1]}^T,...,\alpha_k\textbf{1}_{[k]}^T,...,\alpha_K\textbf{1}_{[K]}^T],
\end{equation}$$
where $\textbf{1}_{[k]}$ stands for a $J_{[k]}$ vector of ones. Alternatively, the weights can be stored as the diagonal elements of a diagonal matrix denoted by $\textbf{A}$ obtained as
$$\begin{equation}
\textbf{A}=diag\{a\}.
\end{equation}$$

### Step 4. Calculating Cross-Product Matrices
After the weights have been collected, they are used to compute the $GSVD$ of X under the constraints provided by $\textbf{M}$ (masses for the observations) and $\textbf{A}$ (squared singular value derived weights for the $K$ tables). This $GSVD$ is expressed as: 
$$\begin{equation}
\textbf{X}=\textbf{P}\boldsymbol{\Delta}\textbf{Q}^T \quad \text{with} \quad \textbf{P}^T\textbf{M}\textbf{P} = \textbf{Q}^T\textbf{A}\textbf{Q} = \textbf{I}.
\end{equation}$$

In this package we choose an alternative computational approach to compute MFA. That is using the cross-product matrices $\textbf{S}_{[k]}$. The cross-product matrices can be directly computed from $\textbf{X}$ as 
$$\begin{equation}
\textbf{S}_{[+]}=\textbf{X}\textbf{A}\textbf{X}^{\textbf{T}}.
\end{equation}$$
Once we got the cross product matrices, we can do eigendecomposition to get eigenvector and eigenvalue of $\textbf{S}_{[+]}$, denote as $\textbf{U}$ and $\boldsymbol{\lambda}$ respectively.
The generalized eigendecomposition under the constraints provided by matrix $\textbf{M}$ of the compromise gives: 
$$\begin{equation}
\textbf{S}_{[+]}=\textbf{P}\Lambda \textbf{P}^{\textbf{T}} \quad \text{with} \quad \textbf{P}^{\textbf{T}} \textbf{M}\textbf{P}=\textbf{I}.
\end{equation}$$

This equation and the $GSVD$ equation above, together indicate that the generalized eigenvectors of the compromise are the left generalized singular vectors of $\textbf{X}$.The eigenvalues of $\textbf{S}_{[+]}$ (i.e. $\lambda$) are the squares of the singular values of $\textbf{X}$.
By doing eigendecomposition and some matrix transform, we can get :
$\textbf{U}$=eigenvector of $\textbf{S}$  
$$\begin{equation}
\boldsymbol{\Lambda} = \sqrt{\textbf{M}} \lambda \sqrt{\textbf{M}}
\end{equation}$$

$$\begin{equation}
\textbf{P}=\textbf{U}\sqrt{\textbf{M}}^{-1}
\end{equation}$$

$$\begin{equation}
\textbf{Q}=\textbf{X}^\textbf{T}\textbf{M}\textbf{P}\sqrt{\boldsymbol{\Lambda}}^{-1}
\end{equation}$$





We are using a data example included in the package MFA to illustrate the use of the MFA package step by step. This data example concerns 12 wines from three
wine regions and ten expert assessors
were asked to evaluate these wines on 9-point rating
scales, using four standard variables plus extra variables if any aseessor feels necessary.

## 2.1 Getting started
To run any of the MFA functions, it is necessary to make the package active by using the library command:
```{r}
library(MFA)
```
The data on which we apply MFA functions should be either data frame or matrix object in R. The users could of course read data from a local file and here are of course many ways to enter data into R, just make sure prepare the data to be analyzed as data frame or matrix. The data example 'wine' has been loaded in the global environment in R when loading the MFA package, we can use head( ) to check the 'wine' data. As the data example is of many variables, we will choose the first 12 to show:
```
head(wine)[,1:12]
```

It is required that the data on which we apply MFA functions is organized in certain way that the functions could separate each group by the arrangement of the columns of the input data, as shown in the data example 'wine', the variables of each group are stacked together and one group after another, like the first 6 columns are in Group 1 and the next 6 columns are in Group2, and so on. It's also recommended that the input data has row names and columns names for more readable outputs of the MFA functions.
