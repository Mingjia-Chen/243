<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content />

<meta name="date" content="2016-11-29" />

<title>An Overview of the MFA package</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">An Overview of the MFA package</h1>
<h4 class="date"><em>2016-11-29</em></h4>



<p></p>
<p>This vignette gives an introduction to the model Multiple Factor Analysis and an overview the package MFA developed to apply the model on data. Most of this information is available scattered throughout the R documentation. This appendix brings it all together in one place. You might find reading this entire vignette helpful to get a broad understanding of what can be done in R using the MFA.</p>
<div id="introdution-to-multiple-factor-analysis" class="section level2">
<h2>1 Introdution to Multiple Factor Analysis</h2>
<p>Multiple factor analysis (MFA, also called multiple factorial analysis) is an generalization of principal component analysis (PCA). Its goal is to analyze several data sets of variables collected on the same set of observations, or—as in its dual version—several sets of observations measured on the same set of variables.</p>
<p>The goals of MFA are (1) to analyze several data sets measured on the same observations; (2) to provide a set of common factor scores (often called ‘compromise factor scores’); and (3) to project each of the original data sets onto the compromise to analyze communalities and discrepancies.</p>
</div>
<div id="when-to-use-it" class="section level2">
<h2>2 When to Use it</h2>
<p>MFA is used when several sets of variables have been measured on the same set of observations. The number and/or nature of the variables used to describe the observations can vary from one set of variables to the other, but the observations should be the same in all the data sets.</p>
<p>For example, suppose we have 12 wines evaluated by 10 different wine experts. The different data sets can have the same observations (wines) evaluated by different subjects (wine experts) or groups of sub- jects with different variables (each wine expert evaluates the wines with his/her own set of scales). In this case, the first data set corresponds to the first subject, the second one to the second subject and so on. The goal of the analysis, then, is to evaluate if there is an agreement between the subjects or groups of subjects.</p>
</div>
<div id="main-idea" class="section level2">
<h2>3 Main Idea</h2>
<p>The general idea behind MFA is to normalize each of the individual data sets so that their first principal component has the same length. There are several terms with regards to MFA:</p>
<p><strong>compromise/consensus</strong>: Obtained by combining normalized individual data table into a common representation of the observations.</p>
<p><strong>factor scores</strong>: The coordinates of the observations on the components. These can be used to plot maps of the ob- servations.</p>
<p><strong>partial factor scores</strong>: The positions of the observations ‘as seen by’ each data set. These can be also represented as points in the compromise map.</p>
<p><strong>loading</strong>: The quantity that variables contributes a certain amount to each component. This reflects the importance of that variable for this component and can also be used to plot maps of the variables that reflect their association.</p>
<p><strong>contributions</strong>: A variation over squared loadings. These evaluate the importance of each variable as the pro- portion of the explained variance of the component by the variable. The contribution of a data table to a component can be obtained by adding the contributions of its variables. These contributions can then be used to draw plots expressing the importance of the data tables in the common solution.</p>
<div id="svd" class="section level3">
<h3>SVD</h3>
<p>Recall that the <span class="math inline">\(SVD\)</span> of a given <span class="math inline">\(I × J\)</span> matrix <span class="math inline">\(\textbf{Z}\)</span> decomposes it into three matrices as: <span class="math display">\[\begin{equation}
\textbf{X} = \textbf{U} \Gamma \textbf{V}^T\quad \text{with} \quad  \textbf{U}^T\textbf{U} = \textbf{V}^T\textbf{V} = \textbf{I}.
\end{equation}\]</span> This is closely related to and generalizes the well-known <span class="math inline">\(eigendecomposition\)</span> as <span class="math inline">\(\textbf{U}\)</span> is also the matrix of the normalized eigenvectors of <span class="math inline">\(\textbf{X} \textbf{X}^T\)</span>, <span class="math inline">\(\textbf{V}\)</span> is the matrix of the normalized eigenvectors of <span class="math inline">\(\textbf{X}^T \textbf{X}\)</span>. Notice that <span class="math inline">\(\textbf{X} \textbf{X}^T\)</span> is denoted as <span class="math inline">\(\textbf{S}\)</span>, which called cross product matrix of <span class="math inline">\(\textbf{X}\)</span>. Therefore, if we do <span class="math inline">\(eigendecomposition\)</span> for <span class="math inline">\(\textbf{S}\)</span>, we can get eigenvalue <span class="math inline">\(\boldsymbol{\Lambda}\)</span>(i.e. <span class="math inline">\(\Gamma^2\)</span>) and eigenvector <span class="math inline">\(\textbf{U}\)</span> (i.e. the left singular vector of <span class="math inline">\(\textbf{X}\)</span>)</p>
<p><em>Key property</em>: the <span class="math inline">\(SVD\)</span> provides the best reconstitution (in a least squares sense) of the original matrix by a matrix with a lower rank.</p>
</div>
<div id="gsvd" class="section level3">
<h3>GSVD</h3>
<p>The <span class="math inline">\(GSVD\)</span> generalizes the <span class="math inline">\(SVD\)</span> of a matrix by incorporating two additional positive definite matrices that represent ‘constraints’ to be incorporated in the decomposition. We call these two matrices <em>Mass Matrix</em> <span class="math inline">\(\textbf{M}\)</span> representing the ‘constraints’ imposed on the rows of an <span class="math inline">\(I\)</span> by <span class="math inline">\(J\)</span> matrix <span class="math inline">\(\textbf{X}\)</span> and <em>Weight Matrix</em> <span class="math inline">\(\textbf{A}\)</span> representing the ‘constraints’ imposed on the columns of <span class="math inline">\(\textbf{X}\)</span>. Matrix <span class="math inline">\(\textbf{M}\)</span> is almost always a diagonal matrix of the ‘masses’ of the observations (i.e., the rows); whereas matrix <span class="math inline">\(\textbf{A}\)</span> implements a metric on the variables and if often but not always diagonal.Obviously, when <span class="math inline">\(\textbf{M} = \textbf{A} = \textbf{I}\)</span>, the <span class="math inline">\(GSVD\)</span> reduces to the plain <span class="math inline">\(SVD\)</span>.The <span class="math inline">\(GSVD\)</span> of <span class="math inline">\(\textbf{X}\)</span>, taking into account <span class="math inline">\(\textbf{M}\)</span> and <span class="math inline">\(\textbf{A}\)</span>, is expressed as <span class="math display">\[\begin{equation}
\textbf{X}=\textbf{P}\boldsymbol{\Delta}\textbf{Q}^T \quad \text{with} \quad \textbf{P}^T\textbf{M}\textbf{P} = \textbf{Q}^T\textbf{A}\textbf{Q} = \textbf{I}.
\end{equation}\]</span> where <span class="math inline">\(\textbf{P}\)</span> is the <span class="math inline">\(I\)</span> by <span class="math inline">\(L\)</span> matrix of the normalized left generalized singular vectors (with <span class="math inline">\(L\)</span> being the rank of <span class="math inline">\(\textbf{X}\)</span>), <span class="math inline">\(\textbf{Q}\)</span> the <span class="math inline">\(J\)</span> by <span class="math inline">\(L\)</span> matrix of the normalized generalized right singular vectors, and <span class="math inline">\(\boldsymbol{\Delta}\)</span> the <span class="math inline">\(L\)</span> by <span class="math inline">\(L\)</span> diagonal matrix of the <span class="math inline">\(L\)</span> generalized singular values.</p>
<p><em>Key property</em>: the <span class="math inline">\(GSVD\)</span> provides the best reconstitution (in a least squares sense) of the original matrix by a matrix with a lower rank under the constraints imposed by two positive definite matrices. The generalized singular vectors are orthonormal with respect to their respective matrix of constraints.</p>
</div>
</div>
<div id="step-by-step-tutorial" class="section level2">
<h2>4 Step by step tutorial</h2>
<div id="step-1.-preprocesing" class="section level3">
<h3>Step 1. Preprocesing</h3>
<p>Each table was preprocessed by first centering and normalizing each column such that its mean is equal to 0 and the sum of the square values of all its elements is equal to 1. The raw data consist of <span class="math inline">\(K\)</span> data sets collected on the same observations. Each data set is also called a table, a sub-table, or a block. The data for each table are stored in an <span class="math inline">\(I × J_{[k]}\)</span> rectangular data matrix denoted by <span class="math inline">\(\textbf{Y}_{[k]}\)</span>, where <span class="math inline">\(I\)</span> is the number of observations and <span class="math inline">\(J_{[k]}\)</span> the number of variables collected on the observations for the <span class="math inline">\(k\)</span>-th table. The total number of variables is denoted by <span class="math inline">\(J\)</span> (i.e., <span class="math inline">\(J = \sum J_{[k]}\)</span>). Then use <code>scale</code> function to normalize the data. The <span class="math inline">\(K\)</span> data matrices <span class="math inline">\(\textbf{X}_{[k]}\)</span> are concatenated into the complete <span class="math inline">\(I\)</span> by <span class="math inline">\(J\)</span> data matrix denoted by <span class="math inline">\(\textbf{X}\)</span>: <span class="math display">\[\begin{equation}
\textbf{X}=[\textbf{X}_{[1]}|...|\textbf{X}_{[k]}|...|\textbf{X}_{[K]}].
\end{equation}\]</span></p>
</div>
<div id="step-2.-mass-matrix" class="section level3">
<h3>Step 2. Mass Matrix</h3>
<p>A mass, denoted by <span class="math inline">\(m_i\)</span>, is assigned to each observation. These masses are collected in the mass vector, denoted by <span class="math inline">\(\textbf{m}\)</span>, and in the diagonal elements of the mass matrix denoted by <span class="math inline">\(\textbf{M}\)</span>, which is obtained as <span class="math display">\[\begin{equation}
\textbf{M}=\text{diag}\{\textbf{m}\}
\end{equation}\]</span></p>
</div>
<div id="step-3.-weight-matrix" class="section level3">
<h3>Step 3. Weight Matrix</h3>
<p>The weight matrix gathered by doing standard <span class="math inline">\(PCA\)</span> of each Data Table. Specifically, each table is expressed via its <span class="math inline">\(SVD\)</span> as <span class="math display">\[\begin{equation}
\textbf{X}_{[k]} = \textbf{U}_{[k]} \Gamma_{[k]}\textbf{V}^T_{[k]}\quad \text{with} \quad  \textbf{U}^T_{[k]}\textbf{U}_{[k]} = \textbf{V}^T_{[k]}\textbf{V}_{[k]} = \textbf{I}.
\end{equation}\]</span> In MFA, the weight of a table is obtained from the first singular value of its <span class="math inline">\(PCA\)</span>. This weight, denoted by <span class="math inline">\(\alpha_k\)</span>, is equal to the inverse of the first squared singular value: <span class="math display">\[\begin{equation}
\alpha_k=\frac{1}{\gamma_{1,k}^2}=\gamma_{1,k}^{-2}.
\end{equation}\]</span> For convenience, gather <span class="math inline">\(\alpha\)</span> weights in a <span class="math inline">\(J\)</span> by <span class="math inline">\(1\)</span> vector. Specifically, <span class="math inline">\(\textbf{a}\)</span> is constructed as: <span class="math display">\[\begin{equation}
\textbf{a}=[\alpha_1\textbf{1}_{[1]}^T,...,\alpha_k\textbf{1}_{[k]}^T,...,\alpha_K\textbf{1}_{[K]}^T],
\end{equation}\]</span> where <span class="math inline">\(\alpha_k\)</span> stands for the inverse of the first squared singular value of <span class="math inline">\(k\)</span>-th block and <span class="math inline">\(\textbf{1}_{[k]}\)</span> for a <span class="math inline">\(J_{[k]}\)</span> vector of ones. Alternatively, the weights can be stored as the diagonal elements of a diagonal matrix denoted by <span class="math inline">\(\textbf{A}\)</span> obtained as <span class="math display">\[\begin{equation}
\textbf{A}=\textbf{diag}\{\textbf{a}\}.
\end{equation}\]</span></p>
</div>
<div id="step-4.-calculating-cross-product-matrices" class="section level3">
<h3>Step 4. Calculating Cross-Product Matrices</h3>
<p>After the weights have been collected, they are used to compute the <span class="math inline">\(GSVD\)</span> of <span class="math inline">\(\textbf{X}\)</span> under the constraints provided by <span class="math inline">\(\textbf{M}\)</span> and <span class="math inline">\(\textbf{A}\)</span>. In this <span class="math inline">\(MFA\)</span> package, we choose to use the cross-product matrices <span class="math inline">\(\textbf{S}_{[k]}\)</span> to compute all elements of <span class="math inline">\(MFA\)</span>. The cross-product matrices can be directly computed from <span class="math inline">\(\textbf{X}\)</span> as <span class="math display">\[\begin{equation}
\textbf{S}_{[+]}=\textbf{X}\textbf{A}\textbf{X}^{\textbf{T}}.
\end{equation}\]</span></p>
<p>Once we got the cross product matrices, we can do <em>eigendecomposition</em> to get eigenvector and eigenvalue of <span class="math inline">\(\textbf{S}_{[+]}\)</span>, denote as <span class="math inline">\(\textbf{U}\)</span> and <span class="math inline">\(\boldsymbol{\lambda}\)</span> respectively. Actually, <span class="math display">\[\begin{equation}
\textbf{S}_{[+]}=\textbf{U}\boldsymbol{\lambda}\textbf{U}^T=\textbf{P}\sqrt{\textbf{M}}\boldsymbol{\lambda} \sqrt{\textbf{M}}^T\textbf{P}^T=\textbf{P}\boldsymbol{\Lambda}\textbf{P}^T
\end{equation}\]</span></p>
<p>Thus, the generalized <em>eigendecomposition</em> under the constraints provided by matrix <span class="math inline">\(\textbf{M}\)</span> of the compromise gives: <span class="math display">\[\begin{equation}
\textbf{S}_{[+]}=\textbf{P}\Lambda \textbf{P}^{\textbf{T}} \quad \text{with} \quad \textbf{P}^{\textbf{T}} \textbf{M}\textbf{P}=\textbf{I}.
\end{equation}\]</span> Then we can compute eigenvalues <span class="math inline">\(\boldsymbol{\Lambda}\)</span>, left generalized singular vectors <span class="math inline">\(\textbf{P}\)</span> and right generalized singular vectors <span class="math inline">\(\textbf{Q}\)</span> given by:</p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{\Lambda}=\sqrt{\textbf{M}}\boldsymbol{\lambda} \sqrt{\textbf{M}}^T
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\textbf{P}=\textbf{U}\sqrt{\textbf{M}}^{-1}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\textbf{Q}=\textbf{X}^\textbf{T}\textbf{M}\textbf{P}\sqrt{\boldsymbol{\Lambda}}^{-1}
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\lambda}\)</span> and <span class="math inline">\(\textbf{U}\)</span> is the eigenvalue and eigenvector calculated by <code>eigen</code> function on <span class="math inline">\(\textbf{S}_{[+]}\)</span>.</p>
</div>
<div id="step-5.-important-elements-and-methods" class="section level3">
<h3>Step 5. Important elements and methods</h3>
<div id="scores" class="section level4">
<h4>Scores</h4>
<p>Once we got eigenvalue <span class="math inline">\(\boldsymbol{\Lambda}\)</span>, left generalized singular vectors <span class="math inline">\(\textbf{P}\)</span> and right generalized singular vectors <span class="math inline">\(\textbf{Q}\)</span>, we can compute partial factor score <span class="math inline">\(\textbf{F}_{[k]}\)</span> and common factor score <span class="math inline">\(\textbf{F}\)</span> obtained from: <span class="math display">\[\begin{equation}
\textbf{F}_{[k]}=K\alpha_k\textbf{X}_{[k]}\textbf{Q}_{[k]}=K\alpha_k\textbf{X}_{[k]}\textbf{X}_{[k]}^T\textbf{M}\textbf{P}\boldsymbol{\Delta}^{-1}
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\textbf{F}=\frac{1}{K}\sum_k \textbf{F}_{[k]}
\end{equation}\]</span></p>
</div>
<div id="contributions" class="section level4">
<h4>Contributions</h4>
<p>In this part, we calculate three kind of contributions: contribution of an observation to a dimension,contributions of a variable to a dimension and contribution of a table to a dimension.<br />
<strong>Observation</strong><br />
Formally, the contribution of observation <span class="math inline">\(i\)</span> to component <span class="math inline">\(l\)</span>, denoted <span class="math inline">\(ctr_{i,l}\)</span>, is computed as <span class="math display">\[\begin{equation}
ctr_{i,l}=\frac{m_i × f_{i,l}^2}{\lambda_l} \quad \text{with} \quad \lambda_l=\sum_i m_i × f_{i,l}^2
\end{equation}\]</span> where <span class="math inline">\(m_i\)</span> and <span class="math inline">\(f_{i,l}\)</span> are, respectively, the mass of the <span class="math inline">\(i\)</span>th observation and the factor score of the <span class="math inline">\(i\)</span>th observation for the <span class="math inline">\(l\)</span>th dimension.<br />
<strong>Variable</strong><br />
As we did for the observations, we can find the important variables for a given dimension by computing variable contributions.The contribution of variable <span class="math inline">\(j\)</span> to component <span class="math inline">\(l\)</span>, denoted <span class="math inline">\(ctr_{j,l}\)</span>, is computed as <span class="math display">\[\begin{equation}
ctr_{j,l}=a_j × q_{j,l}^2 \quad \text{with} \quad 1=\sum_j a_j × q_{j,l}^2
\end{equation}\]</span> where <span class="math inline">\(q_{i,l}\)</span> is the loading of the <span class="math inline">\(j\)</span>th variable for the <span class="math inline">\(l\)</span>th dimension.<br />
<strong>Table</strong><br />
As a table comprises several variables, the contribution of a table can simply be defined as the sum of the contributions of its variables. So the contribution of table k to component l is denoted <span class="math inline">\(ctr_{k,l}\)</span> and is defined as <span class="math display">\[\begin{equation}
ctr_{k,l}=\sum_j^{J_{[k]}} ctr_{j,l}
\end{equation}\]</span> Contributions take values between 0 and 1, and for a given component, the contributions of all variables sum to 1. The larger a contribution of a <em>observation/variable/table</em> to a component the more this <em>observation/variable/table</em> contributes to this component. Table contributions and partial inertias can be used to create plots that show the importance of these tables for the components. These plots can be drawn one component at a time or two (or rarely three) components at a time in a manner analogous to factor maps.</p>
</div>
<div id="coefficients" class="section level4">
<h4>Coefficients</h4>
<p>To evaluate the similarity between two tables one can compute coefficients of similarity between data tables. There are two kind of coefficients in our <span class="math inline">\(MFA\)</span> package: <span class="math inline">\(R_v\)</span> coefficient reflecting the amount of variance shared by two matrices and <span class="math inline">\(L_g\)</span> coefficient reflecting the MFA normalization and taking positive values. Specifically, <span class="math inline">\(R_V\)</span> coefficient between data tables <span class="math inline">\(k\)</span> and <span class="math inline">\(k′\)</span> is computed as</p>
<p><span class="math display">\[\begin{equation}
R_{Vk,k'}=\frac{\text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}}{\sqrt{\text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k]}\textbf{X}_{[k]}^T)\}× \text{trace}\{(\textbf{X}_{[k']}\textbf{X}_{[k']}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}}}
\end{equation}\]</span> <span class="math inline">\(L_g\)</span> coefficient between data tables <span class="math inline">\(k\)</span> and <span class="math inline">\(k′\)</span> is computed as <span class="math display">\[\begin{equation}
\begin{aligned}
L_{g(k,k')}&amp;=\frac{\text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}}{\gamma_{1,k}^2 × \gamma_{1,k'}^2}\\
&amp;= \text{trace}\{(\textbf{X}_{[k]}\textbf{X}_{[k]}^T)× (\textbf{X}_{[k']}\textbf{X}_{[k']}^T)\}× \alpha_k × \alpha_{k'}
\end{aligned}
\end{equation}\]</span></p>
</div>
<div id="bootstrap-for-factor-scores" class="section level4">
<h4>Bootstrap for Factor Scores</h4>
<p>The bootstrap is using to estimate the stability of the compromise factor scores. The main idea is to use the properties that the compromise factor scores are the average of the partial factor scores. Therefore, we can obtain bootstrap confidence intervals (CIs) by repeatedly sampling with replacement from the set of tables and compute new compromise factor scores. From these estimates we can also compute <strong>bootstrap ratios</strong> for each dimension by dividing the mean of the bootstrap estimates by their standard deviation.<br />
To compute a bootstrap estimate, first we need to generate a bootstrap sample. To do so, we take a sample of integers with replacement from the set of integers from 1 to <span class="math inline">\(K\)</span>. We call this set <span class="math inline">\(\mathbb{B}\)</span> (for bootstrap). Then generate a new data set (i.e., a new <span class="math inline">\(\textbf{X}\)</span> matrix comprising <span class="math inline">\(K\)</span> tables) using matrices <span class="math inline">\(\textbf{X}_{[k]}\)</span> with these indices. Combine all these sample blocks in a data matrix denoted <span class="math inline">\(\textbf{X}_1^*\)</span> that would then be analyzed by <span class="math inline">\(MFA\)</span>. This would provide a set of bootstrapped factor scores (denoted <span class="math inline">\(\textbf{F}_1^*\)</span>). Then repeat the procedure a large number of times (e.g., <span class="math inline">\(L\)</span> = 1000) and generate <span class="math inline">\(L\)</span> bootstrapped matrices of factor scores <span class="math inline">\(\textbf{F}_l^*\)</span> . <span class="math inline">\(\bar{\textbf{F}}^∗\)</span> denotes the bootstrap estimated factor scores, and is computed as <span class="math display">\[\begin{equation}
\bar{\textbf{F}}^∗=\frac{1}{L}\sum_l^L \textbf{F}_l^*
\end{equation}\]</span> <span class="math inline">\(\hat\sigma_{F^*}^2\)</span> denotes the bootstrapped estimate of the variance and is computed as <span class="math display">\[\begin{equation}
\hat\sigma_{F^*}^2=\frac{1}{L}\big(\sum_l^L(\textbf{F}_l^*-\bar{\textbf{F}}^∗) ◦ (\textbf{F}_l^*-\bar{\textbf{F}}^∗)\big)
\end{equation}\]</span> <strong>Bootstrap ratios</strong> is computed as <span class="math display">\[\begin{equation}
\frac{\bar{\textbf{F}}^∗}{\hat\sigma_{F^*}}
\end{equation}\]</span></p>
<pre><code>We are using a data example included in the package MFA to illustrate the use of the MFA package step by step. This data example concerns 12 wines from three
wine regions and ten expert assessors
were asked to evaluate these wines on 9-point rating
scales, using four standard variables plus extra variables if any aseessor feels necessary.</code></pre>
</div>
</div>
</div>
<div id="getting-started" class="section level2">
<h2>2.1 Getting started</h2>
<p>To run any of the MFA functions, it is necessary to make the package active by using the library command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MFA)</code></pre></div>
<p>The data on which we apply MFA functions should be either data frame or matrix object in R. The users could of course read data from a local file and here are of course many ways to enter data into R, just make sure prepare the data to be analyzed as data frame or matrix. The data example ‘wine’ has been loaded in the global environment in R when loading the MFA package, we can use head( ) to check the ‘wine’ data. As the data example is of many variables, we will choose the first 12 to show:</p>
<pre><code>head(wine)[,1:12]</code></pre>
<p>It is required that the data on which we apply MFA functions is organized in certain way that the functions could separate each group by the arrangement of the columns of the input data, as shown in the data example ‘wine’, the variables of each group are stacked together and one group after another, like the first 6 columns are in Group 1 and the next 6 columns are in Group2, and so on. It’s also recommended that the input data has row names and columns names for more readable outputs of the MFA functions.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
